<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Detect Sensitive Data Loss | Prisma AIRS MCP</title>
  <meta name="description" content="Detect and prevent exposure of sensitive data in prompts and responses">
  
  <!-- SEO Tags -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Detect Sensitive Data Loss | Prisma AIRS MCP</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Detect Sensitive Data Loss" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Detect and prevent exposure of sensitive data in prompts and responses" />
<meta property="og:description" content="Detect and prevent exposure of sensitive data in prompts and responses" />
<link rel="canonical" href="http://localhost:4000/prisma-airs/sensitive-data-loss/" />
<meta property="og:url" content="http://localhost:4000/prisma-airs/sensitive-data-loss/" />
<meta property="og:site_name" content="Prisma AIRS MCP" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Detect Sensitive Data Loss" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Detect and prevent exposure of sensitive data in prompts and responses","headline":"Detect Sensitive Data Loss","url":"http://localhost:4000/prisma-airs/sensitive-data-loss/"}</script>
<!-- End Jekyll SEO tag -->

  
  <!-- Styles -->
  <link rel="stylesheet" href="/assets/css/style.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <!-- Rouge syntax highlighting is handled by Jekyll -->
  
  <!-- Favicons -->
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicon-16x16.png">
</head>
<body>
  <nav class="navbar">
    <div class="container">
      <div class="navbar-brand">
        <a href="/" class="navbar-logo">
          <i class="fas fa-shield-alt"></i>
          Prisma AIRS MCP
        </a>
      </div>
      <div class="navbar-menu">
        <a href="/deployment" class="navbar-item ">Deployment</a>
        <a href="/developers" class="navbar-item ">Developers</a>
        <a href="/prisma-airs/overview" class="navbar-item active">Prisma AIRS</a>
        <a href="https://github.com/cdot65/prisma-airs-mcp" class="navbar-item">
          <i class="fab fa-github"></i>
        </a>
      </div>
    </div>
  </nav>

  <div class="documentation-layout">
    <aside class="sidebar">
      
<nav class="sidebar-nav">
  <h4>Getting Started</h4>
  <ul>
    <li><a href="/prisma-airs/overview/" >Overview</a></li>
  </ul>
  
  <h4>Threat Detection</h4>
  <ul>
    <li><a href="/prisma-airs/prompt-injection/" >Detect Prompt Injection</a></li>
    <li><a href="/prisma-airs/malicious-url/" >Detect Malicious URL</a></li>
    <li><a href="/prisma-airs/sensitive-data-loss/" class="active">Detect Sensitive Data Loss</a></li>
    <li><a href="/prisma-airs/mask-sensitive-data/" >Mask Sensitive Data</a></li>
    <li><a href="/prisma-airs/database-security-attack/" >Detect Database Security Attack</a></li>
    <li><a href="/prisma-airs/toxic-content/" >Detect Toxic Content</a></li>
    <li><a href="/prisma-airs/malicious-code/" >Detect Malicious Code</a></li>
    <li><a href="/prisma-airs/ai-agent-threats/" >Detect AI Agent Threats</a></li>
    <li><a href="/prisma-airs/contextual-grounding/" >Detect Contextual Grounding</a></li>
    <li><a href="/prisma-airs/custom-topic-guardrails/" >Custom Topic Guardrails</a></li>
    <li><a href="/prisma-airs/secure-mcp/" >Secure Model Context Protocol</a></li>
  </ul>
</nav>




<style>
.sidebar-nav {
  padding: var(--spacing-md);
}

.sidebar-nav h4 {
  font-size: 0.875rem;
  text-transform: uppercase;
  letter-spacing: 0.05em;
  color: var(--gray);
  margin-bottom: var(--spacing-sm);
  margin-top: var(--spacing-lg);
}

.sidebar-nav h4:first-child {
  margin-top: 0;
}

.sidebar-nav ul {
  list-style: none;
  padding: 0;
  margin: 0 0 var(--spacing-md) 0;
}

.sidebar-nav li {
  margin-bottom: var(--spacing-xs);
}

.sidebar-nav a {
  display: block;
  padding: var(--spacing-xs) var(--spacing-sm);
  color: var(--dark);
  border-radius: var(--border-radius);
  font-size: 0.9375rem;
  transition: all 0.2s ease;
}

.sidebar-nav a:hover {
  background-color: var(--gray-light);
  text-decoration: none;
}

.sidebar-nav a.active {
  background-color: var(--primary);
  color: var(--white);
  font-weight: 500;
}

.sidebar-nav a.active:hover {
  background-color: var(--primary-dark);
}
</style>
    </aside>
    
    <main class="documentation-content">
      <div class="content-header">
        <h1>Detect Sensitive Data Loss</h1>
        
        <p class="lead">Detect and prevent exposure of sensitive data in prompts and responses</p>
        
      </div>
      
      <article class="content">
        <h2 id="overview">Overview</h2>

<p>Detect and prevent exposure of sensitive data such as API keys, credit card numbers, and PII in prompts and responses.</p>

<h2 id="the-risk-of-data-exposure">The Risk of Data Exposure</h2>

<p>AI systems can inadvertently expose sensitive information through:</p>

<ul>
  <li>Training data leakage</li>
  <li>User input containing secrets</li>
  <li>Generated code with embedded credentials</li>
  <li>Responses that include private data</li>
  <li>Context retention of sensitive information</li>
</ul>

<h2 id="detection-capabilities">Detection Capabilities</h2>

<h3 id="data-types-protected">Data Types Protected</h3>

<h4 id="financial-information">Financial Information</h4>
<ul>
  <li><strong>Credit Card Numbers</strong>: All major card types</li>
  <li><strong>Bank Account Numbers</strong>: Domestic and IBAN</li>
  <li><strong>Routing Numbers</strong>: ACH and wire transfers</li>
  <li><strong>Cryptocurrency Keys</strong>: Private keys and seeds</li>
</ul>

<h4 id="personal-identifiable-information-pii">Personal Identifiable Information (PII)</h4>
<ul>
  <li><strong>Social Security Numbers</strong>: US SSN format</li>
  <li><strong>National IDs</strong>: 50+ country formats</li>
  <li><strong>Passport Numbers</strong>: International formats</li>
  <li><strong>Driverâ€™s License</strong>: State and country specific</li>
</ul>

<h4 id="authentication-credentials">Authentication Credentials</h4>
<ul>
  <li><strong>API Keys</strong>: 100+ service patterns</li>
  <li><strong>Passwords</strong>: Plain text passwords</li>
  <li><strong>OAuth Tokens</strong>: Bearer tokens and secrets</li>
  <li><strong>Database Credentials</strong>: Connection strings</li>
</ul>

<h4 id="healthcare-data-phi">Healthcare Data (PHI)</h4>
<ul>
  <li><strong>Medical Record Numbers</strong>: MRN patterns</li>
  <li><strong>Insurance IDs</strong>: Policy numbers</li>
  <li><strong>Health Information</strong>: Diagnoses, medications</li>
  <li><strong>HIPAA Identifiers</strong>: All 18 types</li>
</ul>

<h2 id="api-example">API Example</h2>

<h3 id="request-format">Request Format</h3>

<p>The following cURL request demonstrates scanning for sensitive data:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-L</span> <span class="s1">'https://service.api.aisecurity.paloaltonetworks.com/v1/scan/sync/request'</span> <span class="se">\</span>
<span class="nt">--header</span> <span class="s1">'Content-Type: application/json'</span> <span class="se">\</span>
<span class="nt">--header</span> <span class="s1">'x-pan-token: &lt;your-API-key&gt;'</span> <span class="se">\</span>
<span class="nt">--header</span> <span class="s1">'Accept: application/json'</span> <span class="se">\</span>
<span class="nt">--data</span> <span class="s1">'{
  "tr_id": "1234",
  "ai_profile": {
    "profile_name": "aisec-profile"
  },
  "metadata": {
    "app_user": "test-user-1",
    "ai_model": "Test AI model"
  },
  "contents": [
    {
      "prompt": "bank account 8775664322 routing number 2344567 dNFYiMZqQrLH35YIsEdgh2OXRXBiE7Ko1lR1nVoiJsUXdJ2T2xiT1gzL8w 6011111111111117 K sfAC3S4qB3b7tP73QBPqbHH0m9rvdcrMdmpI gbpQnQNfhmHaDRLdvrLoWTeDtx9qik0pB68UgOHbHJW7ZpU1ktK7A58icaCZWDlzL6UKswxi8t4z3 x1nK4PCsseq94a02GL7f7KkxCy7gkzfEqPWdF4UBexP1JM3BGMlTzDKb2",
      "response": "This is a test response"
    }
  ]
}'</span>
</code></pre></div></div>

<h3 id="response-format">Response Format</h3>

<p>When sensitive data is detected, the API returns:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"action"</span><span class="p">:</span><span class="w"> </span><span class="s2">"block"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"category"</span><span class="p">:</span><span class="w"> </span><span class="s2">"malicious"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"profile_name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"aisec-profile-demo"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"prompt_detected"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"dlp"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span><span class="w">
    </span><span class="nl">"injection"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w">
    </span><span class="nl">"url_cats"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"report_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"R00000000-0000-0000-0000-000000000000"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"response_detected"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"dlp"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w">
    </span><span class="nl">"url_cats"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"scan_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"00000000-0000-0000-0000-000000000000"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"tr_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"1234"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p><strong>Key Response Fields</strong>:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">prompt_detected.dlp</code>: <code class="language-plaintext highlighter-rouge">true</code> indicates sensitive data was detected in the prompt</li>
  <li><code class="language-plaintext highlighter-rouge">response_detected.dlp</code>: <code class="language-plaintext highlighter-rouge">true</code> indicates sensitive data was detected in the response</li>
  <li><code class="language-plaintext highlighter-rouge">category</code>: Set to <code class="language-plaintext highlighter-rouge">"malicious"</code> when DLP match found, <code class="language-plaintext highlighter-rouge">"benign"</code> otherwise</li>
  <li><code class="language-plaintext highlighter-rouge">action</code>: Based on your AI security profile settings (e.g., <code class="language-plaintext highlighter-rouge">"block"</code>)</li>
</ul>

<p><strong>Note</strong>: Enable Sensitive Data Detection with Basic or Advanced options in the API security profile for this detection.</p>

<h2 id="common-sensitive-data-patterns">Common Sensitive Data Patterns</h2>

<h3 id="financial-data">Financial Data</h3>
<ul>
  <li>Bank account numbers with routing numbers</li>
  <li>Credit card numbers (all major formats)</li>
  <li>SWIFT/BIC codes</li>
  <li>Cryptocurrency private keys</li>
</ul>

<h3 id="authentication-secrets">Authentication Secrets</h3>
<ul>
  <li>API keys and tokens</li>
  <li>OAuth credentials</li>
  <li>JWT tokens</li>
  <li>Database connection strings</li>
</ul>

<h3 id="personal-information">Personal Information</h3>
<ul>
  <li>Social Security Numbers</li>
  <li>Passport numbers</li>
  <li>Driverâ€™s license numbers</li>
  <li>National identification numbers</li>
</ul>

<h2 id="response-actions">Response Actions</h2>

<p>The specific action shown in the response is based on your AI security profile settings:</p>

<ol>
  <li><strong>Block</strong>: Prevent processing when sensitive data detected</li>
  <li><strong>Mask</strong>: Redact sensitive information (if configured)</li>
  <li><strong>Alert</strong>: Log detection for security monitoring</li>
  <li><strong>Allow with Warning</strong>: Proceed with caution flag</li>
</ol>

<h2 id="performance-considerations">Performance Considerations</h2>

<ul>
  <li><strong>Pattern Matching</strong>: High-speed regex and ML-based detection</li>
  <li><strong>False Positive Tuning</strong>: Configurable sensitivity levels</li>
  <li><strong>Batch Processing</strong>: Efficient scanning of large payloads</li>
  <li><strong>Real-time Detection</strong>: Minimal latency impact</li>
</ul>

<h2 id="related-resources">Related Resources</h2>

<ul>
  <li><a href="/developers/api">API Reference</a></li>
  <li><a href="/prisma-airs/overview">Overview</a></li>
  <li><a href="/prisma-airs/mask-sensitive-data">Mask Sensitive Data</a></li>
</ul>

      </article>
      
      <div class="content-footer">
        <div class="edit-page">
          <a href="https://github.com/cdot65/prisma-airs-mcp/edit/main/docs/_prisma-airs/sensitive-data-loss.md">
            <i class="fas fa-edit"></i> Edit this page on GitHub
          </a>
        </div>
        
        
        <div class="pagination">
          
          
          <a href="/prisma-airs/toxic-content/" class="next">
            Detect Toxic Content <i class="fas fa-arrow-right"></i>
          </a>
          
        </div>
        
      </div>
    </main>
    
    <aside class="toc">
      <h4>On this page</h4>
      <nav id="table-of-contents"></nav>
    </aside>
  </div>

  <!-- Syntax highlighting is handled by Jekyll Rouge -->
  <script src="/assets/js/main.js"></script>
</body>
</html>