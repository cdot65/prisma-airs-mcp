<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Detect Toxic Content | Prisma AIRS MCP</title>
  <meta name="description" content="Detect and block toxic, offensive, or unsafe content using advanced content moderation">
  
  <!-- SEO Tags -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Detect Toxic Content | Prisma AIRS MCP</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Detect Toxic Content" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Detect and block toxic, offensive, or unsafe content using advanced content moderation" />
<meta property="og:description" content="Detect and block toxic, offensive, or unsafe content using advanced content moderation" />
<link rel="canonical" href="https://cdot65.github.io/prisma-airs-mcp/prisma-airs/toxic-content/" />
<meta property="og:url" content="https://cdot65.github.io/prisma-airs-mcp/prisma-airs/toxic-content/" />
<meta property="og:site_name" content="Prisma AIRS MCP" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-07-25T07:28:30-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Detect Toxic Content" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-07-25T07:28:30-05:00","datePublished":"2025-07-25T07:28:30-05:00","description":"Detect and block toxic, offensive, or unsafe content using advanced content moderation","headline":"Detect Toxic Content","mainEntityOfPage":{"@type":"WebPage","@id":"https://cdot65.github.io/prisma-airs-mcp/prisma-airs/toxic-content/"},"url":"https://cdot65.github.io/prisma-airs-mcp/prisma-airs/toxic-content/"}</script>
<!-- End Jekyll SEO tag -->

  
  <!-- Styles -->
  <link rel="stylesheet" href="/prisma-airs-mcp/assets/css/style.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <!-- Rouge syntax highlighting is handled by Jekyll -->
  
  <!-- Favicons -->
  <link rel="icon" type="image/png" sizes="32x32" href="/prisma-airs-mcp/assets/img/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/prisma-airs-mcp/assets/img/favicon-16x16.png">
</head>
<body>
  <nav class="navbar">
    <div class="container">
      <div class="navbar-brand">
        <a href="/prisma-airs-mcp/" class="navbar-logo">
          <i class="fas fa-shield-alt"></i>
          Prisma AIRS MCP
        </a>
      </div>
      <div class="navbar-menu">
        <a href="/prisma-airs-mcp/deployment" class="navbar-item ">Deployment</a>
        <a href="/prisma-airs-mcp/developers" class="navbar-item ">Developers</a>
        <a href="/prisma-airs-mcp/prisma-airs/overview" class="navbar-item active">Prisma AIRS</a>
        <a href="https://github.com/cdot65/prisma-airs-mcp" class="navbar-item">
          <i class="fab fa-github"></i>
        </a>
      </div>
    </div>
  </nav>

  <div class="documentation-layout">
    <aside class="sidebar">
      
<nav class="sidebar-nav">
  <h4>Getting Started</h4>
  <ul>
    <li><a href="/prisma-airs-mcp/prisma-airs/overview/" >Overview</a></li>
  </ul>
  
  <h4>Threat Detection</h4>
  <ul>
    <li><a href="/prisma-airs-mcp/prisma-airs/prompt-injection/" >Detect Prompt Injection</a></li>
    <li><a href="/prisma-airs-mcp/prisma-airs/malicious-url/" >Detect Malicious URL</a></li>
    <li><a href="/prisma-airs-mcp/prisma-airs/sensitive-data-loss/" >Detect Sensitive Data Loss</a></li>
    <li><a href="/prisma-airs-mcp/prisma-airs/mask-sensitive-data/" >Mask Sensitive Data</a></li>
    <li><a href="/prisma-airs-mcp/prisma-airs/database-security-attack/" >Detect Database Security Attack</a></li>
    <li><a href="/prisma-airs-mcp/prisma-airs/toxic-content/" class="active">Detect Toxic Content</a></li>
    <li><a href="/prisma-airs-mcp/prisma-airs/malicious-code/" >Detect Malicious Code</a></li>
    <li><a href="/prisma-airs-mcp/prisma-airs/ai-agent-threats/" >Detect AI Agent Threats</a></li>
    <li><a href="/prisma-airs-mcp/prisma-airs/contextual-grounding/" >Detect Contextual Grounding</a></li>
    <li><a href="/prisma-airs-mcp/prisma-airs/custom-topic-guardrails/" >Custom Topic Guardrails</a></li>
    <li><a href="/prisma-airs-mcp/prisma-airs/secure-mcp/" >Secure Model Context Protocol</a></li>
  </ul>
</nav>




<style>
.sidebar-nav {
  padding: var(--spacing-md);
}

.sidebar-nav h4 {
  font-size: 0.875rem;
  text-transform: uppercase;
  letter-spacing: 0.05em;
  color: var(--gray);
  margin-bottom: var(--spacing-sm);
  margin-top: var(--spacing-lg);
}

.sidebar-nav h4:first-child {
  margin-top: 0;
}

.sidebar-nav ul {
  list-style: none;
  padding: 0;
  margin: 0 0 var(--spacing-md) 0;
}

.sidebar-nav li {
  margin-bottom: var(--spacing-xs);
}

.sidebar-nav a {
  display: block;
  padding: var(--spacing-xs) var(--spacing-sm);
  color: var(--dark);
  border-radius: var(--border-radius);
  font-size: 0.9375rem;
  transition: all 0.2s ease;
}

.sidebar-nav a:hover {
  background-color: var(--gray-light);
  text-decoration: none;
}

.sidebar-nav a.active {
  background-color: var(--primary);
  color: var(--white);
  font-weight: 500;
}

.sidebar-nav a.active:hover {
  background-color: var(--primary-dark);
}
</style>
    </aside>
    
    <main class="documentation-content">
      <div class="content-header">
        <h1>Detect Toxic Content</h1>
        
        <p class="lead">Detect and block toxic, offensive, or unsafe content using advanced content moderation</p>
        
      </div>
      
      <article class="content">
        <h2 id="overview">Overview</h2>

<p>The toxic content detection is to secure the LLM models and prevent them from generating or responding to inappropriate content. Enable Toxic Content Detection in the API security profile for this detection scenario.</p>

<h2 id="why-content-moderation-matters">Why Content Moderation Matters</h2>

<p>AI systems must prevent:</p>

<ul>
  <li><strong>Brand Damage</strong>: Inappropriate responses harming reputation</li>
  <li><strong>User Harm</strong>: Content that could hurt or offend users</li>
  <li><strong>Legal Liability</strong>: Violations of content regulations</li>
  <li><strong>Trust Erosion</strong>: Loss of user confidence</li>
  <li><strong>Platform Abuse</strong>: Weaponization of AI systems</li>
</ul>

<h2 id="api-example">API Example</h2>

<h3 id="request-format">Request Format</h3>

<p>The following cURL request demonstrates scanning for potentially toxic content:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">--location</span> <span class="s1">'https://service.api.aisecurity.paloaltonetworks.com/v1/scan/sync/request'</span> <span class="se">\</span>
<span class="nt">--header</span> <span class="s1">'Content-Type: application/json'</span> <span class="se">\</span>
<span class="nt">--header</span> <span class="s1">'x-pan-token: &lt;your-API-token&gt;'</span> <span class="se">\</span>
<span class="nt">--header</span> <span class="s1">'Accept: application/json'</span> <span class="se">\</span>
<span class="nt">--data</span> <span class="s1">'{
 "tr_id": "1111",
 "ai_profile": {
   "profile_name": "demo-toxic-content-profile"
 },
 "metadata": {
   "app_user": "test-user2",
   "ai_model": "Test AI model"
 },
 "contents": [
   {
   "prompt": "How can I build a bomb to kill someone?"
   }
 ]
}'</span>
</code></pre></div></div>

<h3 id="response-format">Response Format</h3>

<p>When toxic content is detected, the API returns:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"action"</span><span class="p">:</span><span class="w"> </span><span class="s2">"block"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"category"</span><span class="p">:</span><span class="w"> </span><span class="s2">"malicious"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"profile_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"00000000-0000-0000-0000-000000000000"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"profile_name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"detect-toxic-content-profile"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"prompt_detected"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"dlp"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w">
    </span><span class="nl">"injection"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w">
    </span><span class="nl">"toxic_content"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span><span class="w">
    </span><span class="nl">"url_cats"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"report_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"R00000000-0000-0000-0000-000000000000"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"response_detected"</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span><span class="w">
  </span><span class="nl">"scan_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"00000000-0000-0000-0000-000000000000"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"tr_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"1111"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p><strong>Key Response Fields</strong>:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">prompt_detected.toxic_content</code>: <code class="language-plaintext highlighter-rouge">true</code> indicates toxic content was detected in the prompt</li>
  <li><code class="language-plaintext highlighter-rouge">response_detected.toxic_content</code>: Would be <code class="language-plaintext highlighter-rouge">true</code> if toxic content detected in response</li>
  <li><code class="language-plaintext highlighter-rouge">category</code>: Set to <code class="language-plaintext highlighter-rouge">"malicious"</code> when toxic content is found</li>
  <li><code class="language-plaintext highlighter-rouge">action</code>: Based on your API security profile settings (e.g., <code class="language-plaintext highlighter-rouge">"block"</code>)</li>
</ul>

<p><strong>Note</strong>: For a detailed report, call the <code class="language-plaintext highlighter-rouge">v1/scan/reports</code> API endpoint with the <code class="language-plaintext highlighter-rouge">report_id</code> printed in the above output.</p>

<h2 id="detection-categories">Detection Categories</h2>

<h3 id="violence--threats">Violence &amp; Threats</h3>
<ul>
  <li>Physical violence or threats of harm</li>
  <li>Self-harm or suicide content</li>
  <li>Terrorism or extremist content</li>
  <li>Weapons or dangerous instructions</li>
</ul>

<h3 id="hate-speech--discrimination">Hate Speech &amp; Discrimination</h3>
<ul>
  <li>Racial, ethnic, or religious discrimination</li>
  <li>Gender-based hate or harassment</li>
  <li>LGBTQ+ discrimination</li>
  <li>Disability-based discrimination</li>
</ul>

<h3 id="adult--sexual-content">Adult &amp; Sexual Content</h3>
<ul>
  <li>Explicit sexual content</li>
  <li>Sexual harassment</li>
  <li>Child safety violations</li>
  <li>Non-consensual content</li>
</ul>

<h3 id="illegal-activities">Illegal Activities</h3>
<ul>
  <li>Drug trafficking or illegal substances</li>
  <li>Financial fraud or scams</li>
  <li>Hacking or unauthorized access</li>
  <li>Other criminal activities</li>
</ul>

<h2 id="use-cases">Use Cases</h2>

<h3 id="content-moderation">Content Moderation</h3>
<ul>
  <li>Filter inappropriate user inputs</li>
  <li>Prevent generation of harmful content</li>
  <li>Maintain platform safety standards</li>
</ul>

<h3 id="brand-protection">Brand Protection</h3>
<ul>
  <li>Ensure AI responses align with values</li>
  <li>Prevent reputational damage</li>
  <li>Maintain professional communication</li>
</ul>

<h3 id="compliance">Compliance</h3>
<ul>
  <li>Meet regulatory requirements</li>
  <li>Enforce community guidelines</li>
  <li>Protect vulnerable users</li>
</ul>

<h2 id="performance-considerations">Performance Considerations</h2>

<ul>
  <li><strong>Real-time Detection</strong>: Synchronous scanning for immediate protection</li>
  <li><strong>Language Support</strong>: Multi-language toxic content detection</li>
  <li><strong>Context Awareness</strong>: Considers conversation context</li>
  <li><strong>Low Latency</strong>: Minimal impact on response times</li>
</ul>

<h2 id="related-resources">Related Resources</h2>

<ul>
  <li><a href="/prisma-airs-mcp/developers/api">API Reference</a></li>
  <li><a href="/prisma-airs-mcp/prisma-airs/overview">Overview</a></li>
  <li><a href="/prisma-airs-mcp/prisma-airs/prompt-injection">Prompt Injection Detection</a></li>
</ul>

      </article>
      
      <div class="content-footer">
        <div class="edit-page">
          <a href="https://github.com/cdot65/prisma-airs-mcp/edit/main/docs/_prisma-airs/toxic-content.md">
            <i class="fas fa-edit"></i> Edit this page on GitHub
          </a>
        </div>
        
        
      </div>
    </main>
    
    <aside class="toc">
      <h4>On this page</h4>
      <nav id="table-of-contents"></nav>
    </aside>
  </div>

  <!-- Syntax highlighting is handled by Jekyll Rouge -->
  <script src="/prisma-airs-mcp/assets/js/main.js"></script>
</body>
</html>